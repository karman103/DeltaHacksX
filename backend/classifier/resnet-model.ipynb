{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae5e6ed0",
   "metadata": {},
   "source": [
    "# Using FastAI for creating a Waste Classifier\n",
    "\n",
    "Fastai is a deep learning library which provides high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains. \n",
    "\n",
    "It has two main design goals: \n",
    "1. To be approachable and rapidly productive\n",
    "2. To be also configurable. \n",
    "\n",
    "## Waste Classifier\n",
    "The aim is to build a model for waste classification that identifies among the different classes: \n",
    "- cardboard\n",
    "- compost\n",
    "- glass\n",
    "- metal\n",
    "- paper\n",
    "- plastic\n",
    "- trash\n",
    "\n",
    "This machine learning model will help people to improve their decision when classifying trash   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585dec66",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52878dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import load_learner, vision_learner, resnet50, RandomResizedCrop, ToTensor, aug_transforms\n",
    "from fastai.metrics import accuracy\n",
    "\n",
    "from fastai.vision.data import ImageDataLoaders\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "!pip install mlxtend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec93f4d",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "The data is already splitted in train and test folders. Inside each folder contains one folder for each class. Those images were obtained using Bing searcher using the api HTTP.   \n",
    "Those images have been manually cleaned, removing the ones that were not useful or were in the wrong category. \n",
    "The data has been divided into two sets train and test sets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d8ddbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \".\"\n",
    "DATASET_DIR = f\"{BASE_DIR}/dataset_splits\"\n",
    "TEST_PHOTOS_DIR = f\"{BASE_DIR}/test-photos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651d5625",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $DATASET_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e26313",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $DATASET_DIR/train/glass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975ecec5",
   "metadata": {},
   "source": [
    "## Data augmentation\n",
    "\n",
    "Apply data augmentation to the training data, keeping 10% the for validation set. Data augmentation process apply some transformations (flip, rotate) to the original images to generate more data to train the model. Plus, helps to avoid overfitting, making the training set less specific. \n",
    "\n",
    "The validation set is used to track the model error while training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b21441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data = ImageDataLoaders.from_folder(\n",
    "    DATASET_DIR,\n",
    "    train ='train',\n",
    "    shuffle=True,\n",
    "    valid_pct=0.1,\n",
    "    item_tfms =[ToTensor,RandomResizedCrop(224, min_scale=0.35)],\n",
    "    batch_tfms=aug_transforms(flip_vert=True, batch=True, max_rotate=30.0),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca86804",
   "metadata": {},
   "source": [
    "Let's show some of the data for the validation and training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d433c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.valid_ds.items[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdebed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train_ds.items[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3030e0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e463c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.valid.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bea7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9390228",
   "metadata": {},
   "source": [
    "## Training the model using resnet50\n",
    "\n",
    "ResNet34 is a convolutional neural network(CNN) that has 34 layers. It has been already trained with images from the ImageNet database. It classifies 1000 object from very broad categories, such as pencil or animals. The input size of the network is 224x224. \n",
    "\n",
    "This network can be reused to train other model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d576b013",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learn = vision_learner(data, resnet50, metrics=accuracy)\n",
    "learn.fine_tune(20)\n",
    "MODEL_FILENAME = \"result-resnet50.pkl\"\n",
    "learn.export(fname=f'{BASE_DIR}/{MODEL_FILENAME}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05373437",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "Now, let's calculate the accuracy metric for the test set. \n",
    "The test set refers to a group of data that has been separated form the training data and the model does not know. \n",
    "It contains original images without any transformation. It will reflect how well the model will behave in the real life, when classifying new images.\n",
    "\n",
    "Let's see what is the result for some photos that we took with the cellphone and check what is the output of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83235a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ffcf47",
   "metadata": {},
   "source": [
    "We can see that has good accuracy, so we decided to use this model for our final solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc11282",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learn_loaded = load_learner(f'{BASE_DIR}/{MODEL_FILENAME}')\n",
    "utils.show_predictions(learn_loaded, TEST_PHOTOS_DIR)                      \n",
    "utils.print_results(data.vocab, learn_loaded, f\"{DATASET_DIR}/test\", 'classification_matrix_resnet50.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
